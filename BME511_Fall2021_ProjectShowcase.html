<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">


  <link rel="stylesheet" type="text/css" href="static/css/main.css">
  <!-- Requesting google and other search engine to not index these pages -->
  <meta name="robots" content="noindex">

  <!-- Favicon issues -->
  <link rel="apple-touch-icon" sizes="180x180" href="/ico/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="static/ico/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="static/ico/favicon-16x16.png">
  <link rel="manifest" href="static/ico/site.webmanifest">

  <title>Biomedical Signal Processing</title>
</head>

<body>
  <!-- Navigation -->
  <nav class="navbar navbar-expand-md navbar-light bg-light sticky-top">
    <div class="container">

      <a class="navbar-brand" href="#">
        <img src="static/images/BME511logo.png" width="48" alt="BME 511">
        Biomedical Signal Processing
      </a>

      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ms-auto">
          <li class="nav-item active">
            <a class="nav-link" href="#">Home
                  <span class="sr-only">(current)</span>
            </a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://github.com/haribharadwaj/syllabi/blob/main/PurdueBME511_BiomedSigProc.md" target="_blank">Syllabus</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://github.com/haribharadwaj/syllabi/blob/main/BME511_FinalProjectProposalGuidelines.md" target="_blank">ProjectSpecs</a>
          </li>
        </ul>
      </div>

    </div>
  </nav>
  <img src="static/images/PurdueGateway.jpg" class="img-fluid" alt="Welcome !">

  <!-- Page Content -->
  <main role="main" class="container">
  <h1 class="mt-4">Final Project Showcase</h1>
  <p>  Welcome to the <strong>Final Project Showcase</strong> page for the Fall 2021 edition of <strong>BME 511 Biomedical Signal Processing</strong>, at the <a href="http://engineering.purdue.edu/BME"> Weldon School of Biomedical Engineering </a> at <a href="http://www.purdue.edu/"> Purdue University</a>. BME 511 is a biomedical "data-science" course covering the application of signal processing and stochastic methods to biomedical signals and systems. While orientation to biomedical data is key to this course, the tools and concepts covered provide foundational skills that are useful in many domains. This course is distinct from other classic offerings in ECE/MA/STAT in at least three ways:
  <ol>
      <li> Relevant theory in signal processing and statistical methods is covered as needed, but a major focus is on implementation/application of the fundamental techniques to real-world biomedical signals. </li>
      <li> Statistical methods that are typically taught with a "univariate" perspective are expanded to topologically organized high-dimensional data such as time-series and images, and done so motivated by the needs in biomedical applications (e.g., electrophysiology, neuroimaging). </li>
      <li> This course uses practical applications to integrate probabilistic methods (e.g., estimation, detection, filtering) with classic linear-algebraic tools (e.g., Fourier/wavelet transforms, PCA, noise cancellation/regression). These foundational areas are often introduced in separate courses, but are powerful when brought together.</li>
  </ol>
  A "hands-on" approach is taken throughout the course via application-oriented problem sets. The course also includes a pre-defined <a href="https://nbviewer.org/github/haribharadwaj/notebooks/blob/main/BME511/MidtermProject.ipynb" target="_blank">midterm project (brain-computer interface)</a>, and a self-selected final project (last 5 weeks). An important goal of the final project is that each student should gain <i>independent</i> experience applying the tools/concepts to a real-world application of their interest. For more information about the overall course, please see the <a href="https://github.com/haribharadwaj/syllabi/blob/main/PurdueBME511_BiomedSigProc.md" target="_blank">syllabus</a>.  For more details about the project, please see the <a href="https://github.com/haribharadwaj/syllabi/blob/main/BME511_FinalProjectProposalGuidelines.md" target="_blank">project proposal specifications.</a> Student final projects are showcased below with their permission.
  </p>

  <h2 class="mt-4">List of projects in no particular order <span class="text-muted">(Click each to expand)</span></h2>
  <div class="accordion" id="accordionExample">

  <div class="accordion-item">
  <h2 class="accordion-header" id="heading3">
    <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse3" aria-expanded="true" aria-controls="collapse3">
      <h6>1. A non-invasive method for evaluating olfactory bulb dysfunction: proof-of-concept for Parkinson’s Disease diagnostics,
      <span class="text-muted ml-2">by Jessica Baer</span>
      </h6>
    </button>
  </h2>
  <div id="collapse3" class="accordion-collapse collapse show" aria-labelledby="heading3" data-bs-parent="#accordionExample">
    <div class="accordion-body">
        <strong>Abstract: </strong> Parkinson’s Disease affects nearly every part of the body as this neurodegenerative disease causes neurons producing the neurotransmitter dopamine to gradually die off, reducing communication levels within the brain. Early, noninvasive diagnostic methods for Parkinson’s Disease (PD) are needed to better manage symptoms and potentially slow disease progression long-term. A recent study by Iravani et al. (2021) shows the potential for using noninvasive electrodes to monitor the common early symptom of olfactory bulb dysfunction and potentially diagnose PD earlier than current PD diagnostic methods that rely on motor symptoms. The objectives of this final project are to confirm the results of this study by identifying features in the electrode recordings that accurately distinguish between PD patients and healthy controls, developing a logistic regression model that uses these features to differentiate between PD patients and healthy controls with reasonable accuracy, and evaluate the correlation between the identified features and disease progression descriptors. To achieve these aims, a total of six key features were isolated from spectrogram representations of the electrode recordings and logistic and linear regression models were fit to the features and the various outcomes. Logistic models using various combinations of features were able to classify the subjects as PD patients and controls with ~95% accuracy. Key features also appeared to be correlated with the three disease progression descriptors that were evaluated.
        <div class="collapse d-flex justify-content-between mt-4">
          <a href="https://www.dropbox.com/s/zjs0lmycmctphla/A%20non-invasive%20method%20for%20evaluating%20olfactory%20bulb%20dysfunction_Presentation.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
            Slides
          </a>
          <a href="https://www.dropbox.com/s/t3idhzl4h4a0mpj/A%20non-invasive%20method%20for%20evaluating%20olfactory%20bulb%20dysfunction_Transcript.pdf?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
            Transcript
          </a>
        </div>
    </div>
    </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading2">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse2" aria-expanded="false" aria-controls="collapse2">
        <h6>2. Prediction of Epileptic Seizures by Spike Extraction and Rate Comparison between Interictal and Preictal States,
        <span class="text-muted ml-2">by Katherine Foote</span>
        </h6>
      </button>
    </h2>
    <div id="collapse2" class="accordion-collapse collapse" aria-labelledby="heading2" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong> Epilepsy, estimated to affect over 50 million people worldwide, is characterized by seizures which occur with little to no advanced external warning. Epilepsy diagnosis from an EEG is generally done by experts by identifying epileptiform discharges, generally characterized as spikes, sharp waves, and other spike-wave type forms. Early detection of an imminent seizure could allow a patient to be in a safe location or possibly take fast-acting seizure preventing medicines, and thus improve their quality of life. Multiple studies have shown that these waveforms are present in increasing quantity during the time leading up to the seizure. This project develops a process to automatically extract spikes and sharp waves in interictal EEG recordings and preictal EEG recordings and count the number of occurrences, generating a smoothed spike rate per observation period. The maximum rate during interictal periods will be calculated and this will be used as a threshold. Other EEG recordings will be processed similarly and compared to the rate of the threshold (per channel). If the EEG recording has a higher rate than the threshold, it indicates that a seizure is imminent. Using this method on a dataset from a Kaggle epilepsy detection competition, the accuracy of prediction for the test subject is above 78%.
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/g6fnmf579f8vhbq/FinalPresentation.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/eqm58oivn2s1goz/video1001558539.mp4?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading1">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse1" aria-expanded="false" aria-controls="collapse1">
        <h6>3. Gait Phase Classification and Prediction,
        <span class="text-muted"> by Tewbesta Alemayehu </span>
        </h6>
      </button>
      
    </h2>
    <div id="collapse1" class="accordion-collapse collapse" aria-labelledby="heading1" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong>Gait is the manner of walking or the analysis of each step when walking. It can be used to determine if the person has abnormalities in their walking and it is important in analyzing how a person should walk if they are missing a limb. There are different phases in gait. These are separated based on time of occurrence. Each gait cycle includes two periods which are stance and swing phase. Stance phase accounts for 60% of the total time for one step and is when both feet are on the ground. Swing phase, accounts for 40% of the total step time and is when the feet is in midair. Stance phase of gait is divided into loading response, midstance, terminal stance, and pre-swing. Swing phase is divided into initial swing, mid-swing, and terminal swing. It is rare to find gait data that has classified each gait phase, although it’s important in understanding the muscular condition of a patient. In this project, one of the objectives was to preprocess EMG data by smoothing it then labelling it the corresponding gait phase associated with it. The second objective was to make predictions of how a person should walk on the other foot given the manner of walking on the right foot. The results show that a good classification was made, and the prediction had a Mean Squared Error of 25.25 and r2 score of 0.29 on a small dataset of one subject walking for less than 40 second.

          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/wglo61r509oc8n4/BME%20Final%20Project%20Presentation%20Slide%20.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a> 
            <a href="https://www.dropbox.com/s/cdoguc2zgu244gs/BME%20Final%20Project%20Presentation%20Slide%20.mp4?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading4">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse4" aria-expanded="false" aria-controls="collapse4">
        <h6>4. Detection of Pulse peaks from wrist Photoplethysmography (PPG) signal using simultaneous chest Electrocardiogram (ECG) signal as ground truth,
        <span class="text-muted ml-2">by Sarwat Amin</span>
        </h6>
      </button>
    </h2>
    <div id="collapse4" class="accordion-collapse collapse" aria-labelledby="heading4" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong> PPG systems can be conveniently built-in wrist watches and rings making it increasingly popular for different applications like heart rate estimation, respiratory rate, and blood pressure estimation. In case of respiratory estimation, it is necessary to detect the pulse peaks within the PPG signal to observe Respiratory Sinus Arrythmia (increase of heart rate during inhalation and decrease during exhalation). However, PPG signal is extremely susceptible to motion artifacts which masks the pulse peaks. To get a robust detection of PPG peak detection, we have implemented and compared the results of 1) bandpass filtering 2) Wavelet decomposition 3) Normalized least mean square (NLMS) adaptive filtering and 4) Recursive least square (RLS) adaptive filtering-based approaches. The PPG signals were acquired from Samsung Galaxy Active2 watch from 28 healthy subjects and simultaneous ECG recordings were acquired from Vitalpatch ECG sensor that has been placed on the chest. The PPG and ECG signals are continuously recorded for 5-7 days, except for the time when the watch was taken off to be charged. The PPG signal is affected by motion artifact induced by a wide range of activities.
          The evaluation of true positive detection is based on the following assumptions:
          1. There is only one true PPG peak between two R-peaks of ECG data.
          2. If the peak detection algorithm detects more than one PPG peaks in between two R-R intervals, the true peak is identified from R-R interval from preceding ECG cycles. The rest of the peaks are labelled as false positive detection.
          The benchmarks of performance are the rate of true positive detections with respect to ECG. The aims for the final project are: Aim 1) reporting the TP rate for all 4 preprocessing techniques for 28 subjects, Aim 2) Lower the false positives in detected PPG peaks, i.e., increase TP rate(PPG) while maintaining a high TP rate (at least 70%) with respect to ECG in 1 subject. The results as of now show that the simplest approach, i.e., bandpass filtering based approach has the best performance with an average TP rate of 53% and TP rate(PPG) of 86%. The reduction of false positive detections using outlier elimination approach did not however work as expected.
          It resulted in an increase in TP rate(PPG) in 2 out of 4 cases but reduced the TP with respect to ECG in all cases. Interestingly, if we modified the peak detection to remove cubing of segments and select appropriate threshold to select outliers, it is possible to get an increase of TP to 62%. So, we have succeeded in fulfilling both the aims of the project.
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/a4m3uw6o1bbutbc/Final%20Project%20Slide%20deck%20vf.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/mlvuxj67bizb3rs/Transcript%20vf.pdf?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Transcript
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading5">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse5" aria-expanded="false" aria-controls="collapse5">
        <h6>5. Arrhythmia Detection using Electrocardiogram,
        <span class="text-muted ml-2">by Rithu Varshini Annadurai</span>
        </h6>
      </button>
    </h2>
    <div id="collapse5" class="accordion-collapse collapse" aria-labelledby="heading5" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong> Arrhythmia detection using the Electrocardiogram(ECG) data is performed
                  through training machine learning models like Logistic Regression and a custom
                  made Deep learning network.Arrhythmia is an abnormality in heartbeat.This is
                  diagnosed by readings taken from EKG,event monitor ,echocardiogram and
                  implantable loop recorder.The data used for this project is from MIT-BIH
                  arrhythmia database.There are 109446 samples with a sampling frequency of
                  125 Hz, also five categories of classes are considered.The classes under the
                  study include, i)Normal beat ii) Supraventricular premature beat, iii)Premature
                  ventricular contraction, iv)Fusion of ventricular and normal beat, v)Unclassifiable
                  beat.First the signal is preprocessed to normalize data and then logistic
                  regression model is applied on the data,as the data is imbalanced, accuracy
                  metric was not able to provide accurate results.Adding nonlinearities to the
                  output of logistic regression,an accuracy of 91% was achievable.After that a
                  custom made neural network was used to achieve a test accuracy of 95.2% with
                  a AUC score( measures the total area underneath the ROC curve) of 0.99. After
                  completion of the model, testing is done on the unseen data in order to quantify
                  the amount of generalization of the model on the data. The accuracy obtained
                  was the same.Also the time consumed for the diagnosis is only seconds.Hence,
                  this simple machine learning method allows a fast retraining of the classifier if
                  new ECG data become available.
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/60x7zlmfx9v09tm/BSP%20Presentation.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/oob1fkh1hihs2jo/BSP%20Final%20Presentation.mov?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading6">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse6" aria-expanded="false" aria-controls="collapse6">
        <h6>6. Predicting music induced emotional responses,
        <span class="text-muted ml-2">by Yvette Espinoza</span>
        </h6>
      </button>
    </h2>
    <div id="collapse6" class="accordion-collapse collapse" aria-labelledby="heading6" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong> Music is known to evoke a range of emotional responses, and thus it can be
                  used for some forms of therapy. In music therapy, the music is chosen to
                  evoke a desired emotion or response, but determining which pieces to play
                  can be challenging, requiring information on both the music and the
                  individual. This project aimed to use a combination of the music’s acoustic
                  properties and EEG measurements to predict music induced emotional
                  responses. Due to time constraints the audio feature extraction was omitted,
                  instead using 2 predefined emotions: ‘sad’ or ‘happy. The participants in the
                  dataset were first presented with different music, labeled with the emotion,
                  then questioned on their emotions. The measured EEG data was
                  preprocessed, and PCA was used to reduce the data and create a classifier to
                  predict the emotional response. The resulting classifier performed poorly,
                  though that was expected because the acoustic features of the music were
                  not included with the EEG data when creating the classifier.
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/a577ri35nxjs251/BME511_Project.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/dvp8mdrv5uduqsp/Espinoza_Project.mp4?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading7">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse7" aria-expanded="false" aria-controls="collapse7">
        <h6>7. Musical pitch and EEG signals,
        <span class="text-muted ml-2">by Trey Bosfield</span>
        </h6>
      </button>
    </h2>
    <div id="collapse7" class="accordion-collapse collapse" aria-labelledby="heading7" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong> Perfect pitch is the rare ability to recognize a note without a reference; relative pitch is the ability to recognize the difference between two or more pitches. Musicians throughout their lifetime
          (those with and without perfect pitch) train their relative pitch, as it helps with harmony, melody,
          and so many more fundamental and advanced concepts that makes music sound the way it
          does. The part of the brain that is responsible for understanding and processing pitch is the
          auditory cortex. The question arose on whether or not it’s possible to compare musicians'
          relative pitch against each other through the use of EEG signals. Experimental eeg raw data
          was taken from Goldman et al. paper on improvisation effect on categorizing musical structures.
          The EEG data of two random subjects in the experiment were preprocessed and compared.
          The results showed that while it was possible to visualize which subject had the better relative
          pitch, it was determined that a classification approach (like AUC or PCA) would be better to
          more easily visualize how good someone’s relative pitch is using EEG data.
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/wyn73l7098mbc9g/Bme%20511%20prsentation%20pdf%20slides.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/s69yy5vdxi9clrz/BME%20511%20final%20project.m4v?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading8">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse8" aria-expanded="false" aria-controls="collapse8">
        <h6>8. Synthetic Segmented Virtual Head Model Generation Using Generative Adversarial Network (GAN),
        <span class="text-muted ml-2">by Nahian Ibn Hasan</span>
        </h6>
      </button>
    </h2>
    <div id="collapse8" class="accordion-collapse collapse" aria-labelledby="heading8" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong> The structural information of the human brain assists in different research such as Transcranial Magnetic Stimulation (TMS), Transcranial Direct Current Stimulation (tDCS), brain tumor detection, uncertainty quantification analysis, etc. However, MRI images of less than 3T resolution are of little use since the tissue boundaries become uncertain in the brain. On the other hand, acquiring a substantial number of higher resolution images (3T, 5T, 7T) is significantly expensive. Hence, an expanded database is often necessary for population-based studies, which is often quite impossible to gather from a single machine, environment, or race of people. Therefore, we aim to develop a workflow to generate synthetic segmented MRI images, which will help in the population-based analysis and machine learning algorithms. Since generating "3D synthetic human head models" is computationally expensive, we aim to work with a single slice of MRI scans. The virtual head models (for ground-truth MRI scans), generated from SimNIBS/Fresurfer, are voxelated to a uniform grid space. Next, we utilize the Generative Adversarial Network (StyleGAN) to produce synthetic segmented MRI slices using over 800 MRI data from Human Connectome Project. The Frechet Inception Distance (FID) score between 5000 generated images and real dataset resembles realistic synthetic data.
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/r57jbpjjejsz07f/Final_Project_Presentation.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/0n85og8y100p4w7/Final_Project_Presentation_Nahian_small.mov?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading9">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse9" aria-expanded="false" aria-controls="collapse9">
        <h6>9. Bayesian Decoding of Hippocampal Cell Assemblies Predicts Location Specific Neural Mapping,
        <span class="text-muted ml-2">by Hammad Khan</span>
        </h6>
      </button>
    </h2>
    <div id="collapse9" class="accordion-collapse collapse" aria-labelledby="heading9" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong>It is well established that the mammalian hippocampus is chiefly responsible for the consolidation of short-term memories and memories associated with spatial navigation. The CA1 region of the hippocampus contains pyramidal neurons that fire in relation to an animal’s position in space. These neurons, termed place cells, fire consistently throughout many behavioral trials. At the population level these neurons capture the spatial field of an environment and can remain stable for longs periods of time. Hence, the neural dynamics of place cells become predictive and reflect synaptic potentiation as a function of behavior. Given the predictive nature of place cell firing, can future locations of the animal be predict based on neural data alone? Here, I train a naïve Bayesian decoder to probabilistically predict an animal’s location in space based on neural data from identified place cells. By developing a multi-step preprocessing pipeline, place cells can be identified from neural recordings of a mouse as it navigates an environment. The Bayesian decoder classified and predicted the location of the animal 60-70% of the time across trials. Although the inherent stochasticity of neural firing rate potentially skewed decoder performance, the comparison of place field prediction was comparable to ground-truth data. Taken together, the dynamic behavior of hippocampal neurons can be captured and predicted using a naïve Bayesian decoder.
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/0vnw9w6d1q137vy/BME511Presentation.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/u6cdpt0a1tbotpi/PresentationBME511.mp4?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading10">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse10" aria-expanded="false" aria-controls="collapse10">
        <h6>10. Common spatial pattern based Motor imagery classification,
        <span class="text-muted ml-2">by Amith Kashyap</span>
        </h6>
      </button>
    </h2>
    <div id="collapse10" class="accordion-collapse collapse" aria-labelledby="heading10" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong> The aim of this project is to understand and decode the EEG signals responsible for the motion of hands and feet in a human subjects. One method of doing so is the use of motor imagery.
          Patterns in brain activity while performing motor imagery are observed in different frequency
          bands for different subjects. This necessitates a subject specific classification scheme. In this
          study we use the method of common spatial patterns(CSP) to extract features from 8 different
          frequency bands(overlapping 4 Hz bands between 4 and 36 Hz) that separates the 8 class motor
          imagery dataset recorded by Schalk G Et Al. Seven different binary classifiers are train to
          differentiate and classify the 8 class problem. The features for each of the 7 classifiers are hand
          crafted for the specific classes that are being classified. The logarithm of the variance of the
          top and bottom 4 sources of all the 8 frequency bands are fed to the classifier for classification.
          The results are quantified by computing the accuracy, precision score, recall score and the f1
          score metrics. The data of each subject was split into train test split and an average accuracy
          of 97.61%. The study also briefly touches on the pitfalls of applying deep learning algorithms
          on motor imagery based dataset.
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/exrvwrwozguxlno/BME_511_Final_Presentation_Amith_Kashyap.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/6ve4a4xn18uzsoj/Final_BME_511_presentation.mp4?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading11">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse11" aria-expanded="false" aria-controls="collapse11">
        <h6>11. Sleep Stage Prediction via EEG Periodogram Frequency Band Energy Classification,
        <span class="text-muted ml-2">by Chris Kannmacher</span>
        </h6>
      </button>
    </h2>
    <div id="collapse11" class="accordion-collapse collapse" aria-labelledby="heading11" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong>An important measure in determining the quality of one’s sleep is determining the stages of sleep one is experiencing in a given night. Currently, sleep scoring is performed by experts on EEG data from the subject during sleep. However, this can cause problems because the results can differ from scorer to scorer. Therefore, there is a need to create a consistent classification model to determine the sleep stages in subjects from EEG data. This project used 6 healthy subjects from the CAP sleep database and created a decision-tree classifier that compared the energy of different frequency bands within the EEG signal. This model differs from others because this model uses the spectral information from the periodogram instead of analyzing just the Fourier transform of the EEG signals. This was done because this project did not assume stationarity in the EEG signal, which should be appropriate because the EEG signal is changing as the subject goes through the sleep cycles. In the end, this model achieved a training accuracy of 65% and testing accuracy of 55% in classifying 6 classes of sleep: awake, stages I – IV, and REM. Due to issues with the consistency of the dataset, preprocessing, and selecting classifiers, this project had its share of difficulties. However, an accuracy of 55% for distinguishing 6 different classes is still significant. Future work includes utilizing more data, perfecting the preprocessing of the data, and experimenting with both the periodogram and Fourier transform to increase accuracy.

          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/oejl5ezblehsbsg/Kannmacher_Presentation_Slides.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/lqxrbcp83r64m5r/Kannmacher_Presentation_Final.mp4?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading12">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse12" aria-expanded="false" aria-controls="collapse12">
        <h6>12. A deep learning framework to remove endogenic artifacts from EEG signals,
        <span class="text-muted ml-2">by Yuqing Huang</span>
        </h6>
      </button>
    </h2>
    <div id="collapse12" class="accordion-collapse collapse" aria-labelledby="heading12" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong> Since endogenic artifacts, such as electrooculogram (EOG) and electromyography (EMG), always contaminate EEG signal, a deep learning framework is proposed to separate EEG signal and endogenic artifacts in the embedding space and reconstruct the denoised signal later. The framework is named DeepSeparator. DeepSeparator employs an encoder to extract and amplify the features in the raw EEG, a module called decomposer to extract the trend, detect and suppress artifact and a decoder to reconstruct the denoised signal. In this project, a set of pure EEG is set as ground truth. Then mix this set of signal with EOG and EMG signals as input. After the process of DeepSeparator, comparing the output with ground truth to show how well the deep learning framework works. Compared with traditional methods, DeepSeparator does not rely on any prior assumptions, and can extract clean EEG.
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/qqy2z9ieu6oynnz/final.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/cn48vfuknqbgwmc/YuqingHuangFinalPre.mp4?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading13">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse13" aria-expanded="false" aria-controls="collapse13">
        <h6>13. Reverse Engineering Single Cell RNA-seq Analysis,
        <span class="text-muted ml-2">by Jonathan Huang</span>
        </h6>
      </button>
    </h2>
    <div id="collapse13" class="accordion-collapse collapse" aria-labelledby="heading13" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong>Genomic data provides valuable insights into the development and characterization of biological and disease processes with DNA sequencing allowing the description of phenotype at the building block level and RNA sequencing giving a window to observe processes in action. Over the last decade, it has been clear that there is missing information lost in tissue heterogeneity that fails to be captured in bulk-sequencing. With the commercialization of single-cell sequencing, there has been an impressive rise in investigators exploring the subpopulations elucidated by the technology. However, the complexity introduced by this granularity requires significantly more resource intensive computational methods. Creation of these methods requires more specialization and as a result, there can be gaps in knowledge between the processing and interpretation of the large datasets. As such, although there are existing methods to process and analyze this type of data, here I seek to reverse engineer these methods to come to a better understanding of the analysis pipelines. 
          494 cells down sampled from a single-cell RNA sequencing data from the caudate will be used through this analysis.
          First a filtering step is necessary due to the technical artifacts present in the sparse data. Next, dimension reduction is required to interpret scRNA data due to the inherent large number of features. After dimension reduction, a shared neighbor algorithm will be used to map the cell clusters.
          Results compared with Seurat show that these steps successfully replicated the traditional scRNA-seq pipeline.

          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/ago0he1a6vpckp3/FProjPresentation.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/aa6kbmuqn9nmwd8/FProjPresentation.mp4?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading14">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse14" aria-expanded="false" aria-controls="collapse14">
        <h6>14. Mouse Paw Sensation and Interpretation: Responses in S1 and M1 Cortex,
        <span class="text-muted ml-2">by Myriam Hrosz</span>
        </h6>
      </button>
    </h2>
    <div id="collapse14" class="accordion-collapse collapse" aria-labelledby="heading14" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong> Animals respond to environmental changes through their perceived notion of the world. Thus, understanding how different regions of the brain communicate to effectively adapt to environmental factors is important. Genetically modified mice (GCaMP6s x CamKII-tTa) were placed on a wheel with three different textures, 40 minutes of behavioral and neural data were recorded. The initial hypothesis was that mice would prefer a smoother surface due to less neurons firing in S1. This in turn would lead to less firing in M1. After processing in DeepLabCut and Suite2p, respectively, a correlation between firing rates and different sensory stimuli was noticed. Mice had a stronger preference for 320-grit sandpaper due to smaller firing rates in S1 cortex. Goal: Determine neural preference in both M1 and S1 in relation to different sensory stimuli.

          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/tibtaant4mcnodh/Final%20Presentation.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/mc0p5omqv74m8qs/video1628527724.mp4?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading15">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse15" aria-expanded="false" aria-controls="collapse15">
        <h6>15. Feasibility of Principal Component Analysis (PCA) for Non-Real Time Detection of Abnormal Heart Rhythms (Arrhythmias),
        <span class="text-muted ml-2">by Amy Hostetler</span>
        </h6>
      </button>
    </h2>
    <div id="collapse15" class="accordion-collapse collapse" aria-labelledby="heading15" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong> To analyze an electrocardiogram, ECG, of a patient to detect abnormal heart rhythms, an expert is needed to diagnose the patients. There is potential for signal processing to be done after an ECG is captured to classify any abnormal heart beats in the data. This project uses the Arrhythmia Database from MIT-BIH by George Moody and Roger Mark which was first distributed in 1980. This is a collection of 47 subjects with a combination of normal heart rhythms and arrhythmias. Moody’s WFDB conversion software was used to create readable ECG data. This project filtered these ECGs to pass the frequencies between 0.5 to 150 Hz based on recommendations from the American Heart Association published in 2007. The data was separated into sections of the ECGs with the same classifications of arrhythmias or normal heart rhythms. These were then dimensionally reduced using a short sample of the data which seemed to have the most difference between the arrhythmias and normal heart rhythm. These were further reduced using PCA to find the most variable components. The resulting criteria for finding specific classifications of arrhythmias show some promise of the application of these methods on other classifications of arrhythmias if the selection criteria between normal and the arrhythmia is more refined. With ventricular flutter wave, there is a 68% false detection of ventricular flutter with normal heart beats.
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/37htc93v2ququdm/Final%20Project%20Presentation.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/yy2136849uqx10n/Final%20Project%20Presentation.mp4?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading16">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse2" aria-expanded="false" aria-controls="collapse2">
        <h6>16. Comparison between measures of Cortical Waveforms and Inter-trial Coherence (ITC) to three different gap durations in younger and middle-aged adults,
        <span class="text-muted ml-2">by Varsha Mysore Athreya</span>
        </h6>
      </button>
    </h2>
    <div id="collapse2" class="accordion-collapse collapse" aria-labelledby="heading16" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong> Many individuals with normal hearing thresholds have difficulty in communication in adverse listening conditions, like in noise or in reverberation. Such difficulties have been majorly documented in older adults, and individuals exposed to noise. There are several factors that aids speech perception in noise, and one of the primary contributors is precise temporal coding of the stimulus. To study temporal coding, we are using a gap detection paradigm of tones with gap durations of 16, 32 and 64 ms in younger and older adults. I have used 32-channel EEG data from 5 younger adults (18-30 years) and 1 middle-aged adult (47 years) to elicit an auditory cortical response. The signal processing techniques used to analyze the cortical response from the raw EEG data were – (1) re-referencing to the mastoid electrodes, (2) band-pass filtering (1-40 Hz), (3) Epoching the three gap durations separately by using 3 triggers, (4) Artifact rejection to remove eye blinks and other muscle-related artifacts, (5) Averaging the three different epochs and (6) Inter-trial coherence (ITC) measures using multitaper method to elucidate the extent of phase-locking for the three gap-durations (giving us a measure of cortical synchrony). I obtained the following responses from the analysis – (1) Onset cortical response to the start of the stimulus, (2) Cortical responses to the three different gap durations, and (3) ITC measures across the three gap durations. The ITC increased with increase in the gap durations, which was not evident in the cortical response waveforms.
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/pjvhfx7wmapt66u/BME%20511-%20Final%20Project%20%28Varsha%29.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/yzcksayjfb9xqnb/Varsha_BME511_FinalProject_Presentation.mp4?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading17">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse17" aria-expanded="false" aria-controls="collapse17">
        <h6>17. Motion Artifact Removal from EEG and fNIRS Data,
        <span class="text-muted ml-2">by Peter Zoss</span>
        </h6>
      </button>
    </h2>
    <div id="collapse17" class="accordion-collapse collapse" aria-labelledby="heading17" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong>The analysis of EEG and fNIRS data for the removal of motion artifacts is important because of the noise they can cause to the raw neural data. To better understand and analyze the data, it is necessary to accurately remove the motion artifacts without losing important information. This is especially true for EEG and fNIRS data where eye movement is more easily detected when recording these signals. The filtering technique I use is able to remove the motion artifacts from the raw data signals and showing what the predicted motion artifact data should look like. The theoretical motion artifact data is then obtained for comparison to the predicted motion signal from the filtered data. The similarities between the two signals are high, showing the potential this technique has.
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/7t13993t36hvezf/BME511_FinalPresentation_pzoss.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/4k286ltic13kvc4/BME511_FinalPresentation_pzoss.mp4?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading18">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse18" aria-expanded="false" aria-controls="collapse18">
        <h6>18. Fetal HR detection from abdominal ECG,
        <span class="text-muted ml-2">by Damen Wilson</span>
        </h6>
      </button>
    </h2>
    <div id="collapse18" class="accordion-collapse collapse" aria-labelledby="heading18" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong>Fetal heart rhythm arrhythmias are very dangerous, and it is crucial for a caregiver to be aware of such arrythmias for care of the mother and baby. Current solutions are not ideal for continuous monitoring and have limited use cases. By use of a NI-FECG (non-invasive fetal electrocardiogram) it is possible to separate the mother’s ECG waveforms and the fetus’s waveforms. The data set used was collected from 5 women in labor with simultaneous recording of multichannel abdominal ECG and direct inserted invasive fetal ECG (FECG) (ground truth). The main objectives of this were to separate the maternal and fetal ECGs, determine fetal R-peaks and HR, and compare these R-peak detections with the FECG’s R-peaks. First, the data was bandpass filtered and then the first principal component from PCA was used to isolate a clean maternal ECG, and ICA’s first component was used an initial estimate of NI-FECG. Next these two signals were separated using the orthogonality principle. Finally, after wavelet denoising, and 18-35Hz filtering, R-peak detection was performed on the NI-FECG estimate. The analysis pipeline had an average of 81.49% accuracy for R-peaks compared to the FECG recording, 24.00% miss rate, and a 18.35% false alarm rate in detecting false R-peaks. In conclusion, a large proportion of the R-peaks coming from the fetus were detected, but there was a sizeable portion of false and missed peaks as well. Further analysis incorporating improved peak detection and source separation could make this a viable tool in noninvasive fetal ECG monitoring. 
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/7hbas6tkb3ahgqf/Final%20Presentation%20-%20Fetal%20HR%20Detection%20from%20Abdominal%20ECG%20Draft.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/0ahktb7u1bgsfsu/Final%20Presentation%20-%20Fetal%20HR%20Detection%20from%20Abdominal%20ECG%20Draft.pptx?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>

  <div class="accordion-item">
    <h2 class="accordion-header" id="heading19">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse19" aria-expanded="false" aria-controls="collapse19">
        <h6>19. Python Analysis of ECG Signals,
        <span class="text-muted ml-2">by Abigail Van Wormer</span>
        </h6>
      </button>
    </h2>
    <div id="collapse19" class="accordion-collapse collapse" aria-labelledby="heading19" data-bs-parent="#accordionExample">
      <div class="accordion-body">
          <strong>Abstract: </strong>About 1 in every 4 Americans will die from heart disease, making it the leading cause of death in the United States. Electrocardiography, or ECG, signaling is a way to capture information about the heart using electrical signals. ECGs are used to diagnose and track a patient’s heart health. In order to help physicians diagnose heart disease faster and more accurately, a python analysis of ECG signals was performed. One control dataset and one noisy dataset were provided from Paul van Gent for this project. The analysis was performed using the control dataset and included extracting the following information from the raw signal: the R-peaks and the interval in between them, the moving heart rate average, the average heart rate, and the heart rate variability features. Next, a filtering function was created to clean up the noisy ECG signal. However, the signal had prior filtering done by the device it was collected with, so the noisy signal was used to find the previously listed features. Although the filtering function was not used with the noisy dataset, it can be used in the future. Comparing the two datasets, it was found that the control data that was collected from a laying down position had much lower values than the noisy data collected while the person was moving around. This makes sense due to the nature of how the data was collected. Using the code written, physicians have the capability to use this analysis to deliver quick diagnoses of heart disease to patients.
          <div class="collapse d-flex justify-content-between mt-4">
            <a href="https://www.dropbox.com/s/wqtzj8jq32pd4iw/Van%20Wormer%20Final%20Presentation.pdf?dl=0" class="btn btn-outline-secondary btn-sm ml-2 mb-2" target="_blank">
              Slides
            </a>
            <a href="https://www.dropbox.com/s/dn4i6whkwiuf5au/VanWormer_Presentation.mp4?dl=0" class="btn btn-outline-secondary btn-sm mr-2 mb-2" target="_blank">
              Video
            </a>
          </div>
      </div>
    </div>
  </div>



  </main>
  <footer class="pt-4 mt-4  pb-4 text-muted border-top container">
    Created using <a href="https://docs.getpelican.com/en/latest/" target="_blank">Pelican</a> and styled using <a href="https://getbootstrap.com/" target="_blank">Bootstrap 5</a>. &copy;2021 Students and instructors of BME 511. All Rights Reserved.
  </footer>


  <!-- Optional JavaScript -->
  <!-- jQuery first, then Popper.js, then Bootstrap JS -->
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
  <!-- Option 1: Bootstrap Bundle with Popper -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>

  <script src="static/js/main.js" type="text/javascript"> </script>

</body>
